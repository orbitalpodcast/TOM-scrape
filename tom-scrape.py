import requests
from bs4 import BeautifulSoup
import re
import json
import time
import os
import config

def get_episodes(get_path=None, local_file=None):
  # SET UP AND FETCH DATA
  if get_path is None:
    # if no get_path was supplied, start from scratch
    get_path = '/show-notes'
  # fetch data from website (or a local file) and make soup
  if bool(get_path):
    content = get_data_from(get_path).content
  else:
    print('Running off local data')
    with open(local) as f:
      content = f.read()
  soup = BeautifulSoup(content, 'html.parser')
  # parse out the basic items we'll need
  uurl           = soup('a','u-url')
  p_summary      = soup('div', 'p-summary')
  entry_dateline = soup('div', 'entry-dateline')
  # HANDLE DATA
  # Go over each post, build episode attributes, and save them.
  for i, u in enumerate(uurl):
    re_title = re.match(r"(?:Episode )(?P<number>\d+)(?:: )(?P<title>.*)", u.string.strip())
    re_path  = re.match(r"(?:\/show-notes\/)(?P<slug>[^?]*)", u.attrs['href'])
    # find episode name
    if re_title == None:
      ep_number      = -time.time()
      ep_title       = u.string.strip()
    else:
      # handle irregular episode names
      ep_number      = re_title['number']
      ep_title       = re_title['title']
    # find episode description
    if p_summary[i].string:
      ep_description = p_summary[i].string.strip()
    else:
      # handle descriptions with links
      ep_description = p_summary[i].prettify()
    # find episode slug and publish date
    ep_slug          = re_path['slug']
    ep_date          = entry_dateline[1].contents[1].attrs['datetime']
    # let the user know where we are, then save our progress
    print( str(ep_title) )
    with open(config.scraped, 'r+') as f:
      # load data from previous pages
      episodes_dict = json.load(f)
      # add this episode to previous data
      episodes_dict[ep_number] = {'title':        ep_title,
                                  'slug':         ep_slug,
                                  'description':  ep_description,
                                  'publish_date': ep_date}
      # go back to the beginning of the file, and write the updated episode list to the file
      # since we're only ever adding data, we won't ever have orphaned data at the end
      f.seek(0)
      json.dump(episodes_dict, f, indent=2)
  # find the next destination and end this loop
  if bool(get_path):
    # if we're working off the internet, look for a next link
    try:
      next_path = soup('div', 'older')[0].contents[0]['href']
    except IndexError:
      next_path = False
    if next_path:
      # wait a sec, then start a new loop
      time.sleep(config.sleep_interval)
      get_episodes(next_path)
    else:
      print('Done with remote data')
  else:
    # if we ran off local data, don't start a new loop
    print('Done with local data')


def get_details():
  print('Getting details')
  # grab the data from get_episodes()
  with open(config.scraped) as f:
    episodes_dict = json.load(f)
  # iterate over each episode in the list, downloading detailed info from each episode page
  for ep_number in episodes_dict:
    # pull out the URL to visit, fetch data from Squarespace, and make soup
    get_path = '/show-notes/' + episodes_dict[ep_number]['slug']
    content = get_data_from(get_path).content
    soup = BeautifulSoup(content, 'html.parser')
    # find the information we're interested in
    audio_url = soup('div','sqs-audio-embed')[0]['data-url']
    notes = soup('div', 'sqs-block-content')[1].prettify()
    # iterate over all the images, and find the data we're interested in
    images = []
    for child in soup('div', 'image-block-wrapper'):
      image_url =     child.find('img')['src']
      image_caption = child.find('img')['alt']
      images.append( {'url':     image_url,
                      'caption': image_caption} )
    episodes_dict[ep_number].update({'audio_url': audio_url,
                           'notes'    : notes,
                           'images'   : images
                           })
    # save our progress, then wait a sec
    with open(config.scraped, 'r+') as f:
      f.seek(0)
      json.dump(episodes_dict, f, indent=2)
    time.sleep(config.sleep_interval)
  print('Done getting details')

def download_audio():
  print('Downloading audio')
  # grab the scraped data generated by get_episodes()
  with open(config.scraped) as f:
    episodes_dict = json.load(f)
  # iterate over each episode in the list, downloading audio files
  for ep_number, ep_deets in episodes_dict.items():
    # create the episode folder
    try_create(ep_number)
    # have we already downloaded this?
    audio_filename = f'Episode-{ep_number}.mp3'
    if file_exist_already(ep_number, audio_filename):
        continue
    # let the user know where we are, then download the audio
    print(audio_filename)
    audio = get_data_from(get_url=ep_deets['audio_url'])
    # save the file, then wait a sec
    save_file(ep_number, audio_filename, audio)
    time.sleep(config.sleep_interval)
  print('Done downloading audio')

def download_images():
  print('Downloading images')
  # grab the scraped data generated by get_episodes()
  with open(config.scraped) as f:
    episodes_dict = json.load(f)
  # figure out where we are
  # iterate over each episode in the list, downloading audio files
  for ep_number, ep_deets in episodes_dict.items():
    # create the episode folder
    try_create(ep_number)
    # let the user know where we are, then download the images
    print(f"Episode {ep_number}, {len(ep_deets['images'])} image/s")
    for i, image in enumerate(ep_deets['images']):
      image_url = image['url']
      image_extension = re.search(r".[^.]+$", image_url)[0]
      image_filename = f'{ep_number}-{i}{image_extension}'
      image = get_data_from(get_url=image_url)
      # save the file
      save_file(ep_number, image_filename, image)
    # wait a sec
    time.sleep(config.sleep_interval)
  print('Done downloading images')

# HELPERS
def save_file(ep_number, filename, file):
  with open(files+ep_number+'/'+filename, 'w+b') as f:
    f.write(file.content)

def file_exist_already(ep_number, filename):
  os.access(files+ep_number+'/'+filename, os.F_OK)

def try_create(path):
  if not os.access(files+'/'+path, os.F_OK):
    os.makedirs(files+'/'+path)


def get_data_from(get_path=None, get_url=None):
  try: throttle_count
  except NameError: throttle_count = 0
  if get_url is None:
    get_url = config.scrape_domain + get_path
  # try getting data
  print('GET '+ get_url)
  result = requests.get(get_url, 'TOMcast-scraper/1.0')
  # re-try if we got throttled
  while result.status_code == 429:
    # Squarespace doesn't retrn a wait-until in their 429 headers which is real annoying.
    # We're providing a user agent, so we shouldn't get throttled, but worth being polite.
    print('Oops! Throttled.')
    time.sleep(60)
    result = requests.get(config.scrape_domain + get_path)
  return result

# START SCRAPING
# Grab basic episode data from the post listing pages. Pass in a filename to test on local data.
# Don't use prettified output from Soup for your local data! "C:\TOM-scrape\local_data_for_testing.txt"
get_episodes()

# Take the episode list and finish collecting the data from each of the show pages.
# This goes to the internet, and won't run off a backup file.
get_details()

# Take the episode list (with details) and download the audio.
# Ignores original name, for uniformity and simplicity when uploading.
download_audio()

# Take the episode list (with details) and download the audio.
# Ignores the original filename, because Squarespace often has identical names.
download_images()
